"""
çŸ¥è¯†åº“å¼ºä¸€è‡´æ€§åŒæ­¥æœåŠ¡ã€‚

åœ¨åº”ç”¨å¯åŠ¨æ—¶æ‰§è¡Œï¼Œç¡®ä¿æœ¬åœ° PostgreSQL çŸ¥è¯†åº“ä¸ Dify è¿œç«¯çŸ¥è¯†åº“å®Œå…¨ä¸€è‡´ã€‚

åŒæ­¥ç­–ç•¥ï¼ˆä»¥æœ¬åœ° DB ä¸ºä¸»æºï¼ŒDify ä¸ºä»å±ï¼‰ï¼š
  1. æœ¬åœ°æœ‰é›†åˆä½† Dify æ— å¯¹åº” Dataset â†’ åœ¨ Dify åˆ›å»º Dataset å¹¶å›å¡« dify_dataset_id
  2. æœ¬åœ°æœ‰æ–‡ä»¶ä½† Dify æ— å¯¹åº” Document â†’ é‡æ–°ä¸Šä¼ æ–‡ä»¶åˆ° Dify
  3. Dify æœ‰å­¤ç«‹ Datasetï¼ˆæœ¬åœ°æ— å¯¹åº”é›†åˆï¼‰â†’ ä» Dify åˆ é™¤
  4. Dify æœ‰å­¤ç«‹ Documentï¼ˆæœ¬åœ°æ— å¯¹åº”æ–‡ä»¶ï¼‰â†’ ä» Dify åˆ é™¤
  5. æ–‡ä»¶çŠ¶æ€ä¸ä¸€è‡´ï¼ˆæœ¬åœ° indexing ä½† Dify å·² completedï¼‰â†’ æ›´æ–°æœ¬åœ°çŠ¶æ€

è®¾è®¡åŸåˆ™ï¼š
  - æœ¬åœ° PostgreSQL æ˜¯å”¯ä¸€çœŸç†æ¥æºï¼ˆSource of Truthï¼‰
  - Dify æ˜¯æ´¾ç”Ÿå­˜å‚¨ï¼Œå¿…é¡»ä¸æœ¬åœ°ä¿æŒä¸€è‡´
  - åŒæ­¥å¤±è´¥ä¸é˜»å¡åº”ç”¨å¯åŠ¨ï¼Œä»…è®°å½•è­¦å‘Š
"""

import asyncio
import logging
from pathlib import Path
from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.database import AsyncSessionLocal
from app.models.knowledge import KBCollection, KBFile
from app.services.dify.factory import get_dify_service
from app.services.dify.base import DifyDatasetItem, DifyDocumentItem

logger = logging.getLogger("govai.kb_sync")


async def sync_kb_with_dify():
    """
    å¯åŠ¨æ—¶çŸ¥è¯†åº“å…¨é‡åŒæ­¥å…¥å£ã€‚
    
    ç¡®ä¿æœ¬åœ° DB ä¸ Dify çš„çŸ¥è¯†åº“æ•°æ®å¼ºä¸€è‡´ã€‚
    ä»»ä½•å•æ­¥å¤±è´¥éƒ½ä¼šè¢«æ•è·å¹¶è®°å½•ï¼Œä¸ä¼šä¸­æ–­æ•´ä½“åŒæ­¥æµç¨‹ã€‚
    """
    dify = get_dify_service()

    # æ£€æµ‹ Dify æ˜¯å¦å¯ç”¨ï¼ˆé Mock æ¨¡å¼æ‰åŒæ­¥ï¼‰
    try:
        dify_datasets = await dify.list_datasets()
    except Exception as e:
        logger.warning(f"âš ï¸  Dify ä¸å¯è¾¾ï¼Œè·³è¿‡çŸ¥è¯†åº“åŒæ­¥: {e}")
        return

    logger.info("ğŸ”„ å¼€å§‹çŸ¥è¯†åº“åŒæ­¥æ£€æŸ¥...")

    dify_dataset_map: dict[str, DifyDatasetItem] = {
        ds.dataset_id: ds for ds in dify_datasets
    }

    async with AsyncSessionLocal() as session:
        # â”€â”€ ç¬¬ä¸€æ­¥ï¼šåŒæ­¥é›†åˆ (Collection â†” Dataset) â”€â”€
        await _sync_collections(session, dify, dify_dataset_map)
        await session.commit()

        # â”€â”€ ç¬¬äºŒæ­¥ï¼šåŒæ­¥æ–‡ä»¶ (KBFile â†” Document) â”€â”€
        await _sync_files(session, dify)
        await session.commit()

        # â”€â”€ ç¬¬ä¸‰æ­¥ï¼šæ¸…ç† Dify å­¤ç«‹ Dataset â”€â”€
        await _cleanup_orphan_datasets(session, dify, dify_dataset_map)

    logger.info("âœ… çŸ¥è¯†åº“åŒæ­¥æ£€æŸ¥å®Œæˆ")


async def _sync_collections(
    session: AsyncSession,
    dify,
    dify_dataset_map: dict[str, DifyDatasetItem],
):
    """
    åŒæ­¥é›†åˆï¼šç¡®ä¿æ¯ä¸ªæœ¬åœ°é›†åˆåœ¨ Dify éƒ½æœ‰å¯¹åº”çš„ Datasetã€‚
    
    åœºæ™¯ï¼š
    - æ•°æ®åº“å·²æŒä¹…åŒ–ï¼Œä½† Dify ä¾§è¢«æ¸…ç†/é‡å»º
    - ç§å­æ•°æ®ä¸­çš„é›†åˆæ²¡æœ‰ dify_dataset_id
    """
    result = await session.execute(select(KBCollection).order_by(KBCollection.created_at))
    collections = result.scalars().all()

    for coll in collections:
        try:
            if not coll.dify_dataset_id:
                # æƒ…å†µ Aï¼šæœ¬åœ°é›†åˆæ²¡æœ‰ dify_dataset_idï¼ˆç§å­æ•°æ®æˆ–å†å²é—ç•™ï¼‰
                logger.info(f"ğŸ“¦ é›†åˆ '{coll.name}' æ—  Dify Datasetï¼Œæ­£åœ¨åˆ›å»º...")
                dataset_info = await dify.create_dataset(coll.name)
                coll.dify_dataset_id = dataset_info.dataset_id
                await session.flush()
                logger.info(f"  âœ… å·²åˆ›å»º Dataset: {dataset_info.dataset_id}")

            elif coll.dify_dataset_id not in dify_dataset_map:
                # æƒ…å†µ Bï¼šæœ¬åœ°æœ‰ dify_dataset_id ä½† Dify ä¸Šä¸å­˜åœ¨ï¼ˆDify ä¾§è¢«åˆ ï¼‰
                logger.warning(
                    f"ğŸ“¦ é›†åˆ '{coll.name}' çš„ Dataset {coll.dify_dataset_id} "
                    f"åœ¨ Dify ä¸­ä¸å­˜åœ¨ï¼Œæ­£åœ¨é‡æ–°åˆ›å»º..."
                )
                old_id = coll.dify_dataset_id
                dataset_info = await dify.create_dataset(coll.name)
                coll.dify_dataset_id = dataset_info.dataset_id
                await session.flush()
                logger.info(
                    f"  âœ… å·²é‡å»º Dataset: {old_id} â†’ {dataset_info.dataset_id}"
                )

                # æ ‡è®°è¯¥é›†åˆä¸‹æ‰€æœ‰æ–‡ä»¶éœ€è¦é‡æ–°ä¸Šä¼ åˆ° Dify
                files_result = await session.execute(
                    select(KBFile).where(KBFile.collection_id == coll.id)
                )
                for f in files_result.scalars().all():
                    f.dify_document_id = None
                    f.dify_batch_id = None
                    if f.status == "indexed":
                        f.status = "indexed"  # ä¿æŒæœ¬åœ°çŠ¶æ€ï¼Œä½†éœ€è¦é‡æ–°ä¸Šä¼ 
                await session.flush()
            else:
                # æƒ…å†µ Cï¼šä¸€åˆ‡æ­£å¸¸
                pass

        except Exception as e:
            logger.error(f"âŒ åŒæ­¥é›†åˆ '{coll.name}' å¤±è´¥: {e}")


async def _sync_files(session: AsyncSession, dify):
    """
    åŒæ­¥æ–‡ä»¶ï¼šç¡®ä¿æ¯ä¸ªæœ¬åœ°å·²ç´¢å¼•çš„æ–‡ä»¶åœ¨ Dify éƒ½æœ‰å¯¹åº”çš„ Documentã€‚
    
    åœºæ™¯ï¼š
    - Dify Dataset è¢«é‡å»ºåï¼Œæ–‡ä»¶éœ€è¦é‡æ–°ä¸Šä¼ 
    - æ–‡ä»¶çŠ¶æ€åœæ»åœ¨ indexing/uploading
    """
    # è·å–æ‰€æœ‰é›†åˆï¼ˆéœ€è¦ dify_dataset_idï¼‰
    coll_result = await session.execute(select(KBCollection))
    collections = {c.id: c for c in coll_result.scalars().all()}

    # è·å–æ‰€æœ‰éœ€è¦åŒæ­¥çš„æ–‡ä»¶
    files_result = await session.execute(select(KBFile).order_by(KBFile.uploaded_at))
    files = files_result.scalars().all()

    # æŒ‰é›†åˆåˆ†ç»„ï¼Œè·å– Dify ä¾§çš„æ–‡æ¡£åˆ—è¡¨ç”¨äºæ¯”å¯¹
    dify_docs_cache: dict[str, dict[str, DifyDocumentItem]] = {}
    
    reupload_count = 0
    status_fix_count = 0

    for f in files:
        coll = collections.get(f.collection_id)
        if not coll or not coll.dify_dataset_id:
            continue  # é›†åˆæ—  dataset_idï¼Œè·³è¿‡

        try:
            dataset_id = coll.dify_dataset_id

            # ç¼“å­˜ Dify æ–‡æ¡£åˆ—è¡¨
            if dataset_id not in dify_docs_cache:
                try:
                    dify_docs = await dify.list_dataset_documents(dataset_id)
                    dify_docs_cache[dataset_id] = {
                        doc.document_id: doc for doc in dify_docs
                    }
                except Exception as e:
                    logger.warning(f"è·å– Dataset {dataset_id} æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
                    dify_docs_cache[dataset_id] = {}

            dify_doc_map = dify_docs_cache[dataset_id]

            # æƒ…å†µ Aï¼šæ–‡ä»¶æœ‰ dify_document_id ä¸”åœ¨ Dify ä¸­å­˜åœ¨ â†’ æ£€æŸ¥çŠ¶æ€
            if f.dify_document_id and f.dify_document_id in dify_doc_map:
                dify_doc = dify_doc_map[f.dify_document_id]
                if f.status == "indexing" and dify_doc.indexing_status == "completed":
                    f.status = "indexed"
                    status_fix_count += 1
                    logger.info(f"  ğŸ“„ ä¿®æ­£æ–‡ä»¶çŠ¶æ€: '{f.name}' indexing â†’ indexed")
                elif f.status == "indexing" and dify_doc.indexing_status == "error":
                    f.status = "failed"
                    f.error_message = "Dify ç´¢å¼•å¤±è´¥ï¼ˆåŒæ­¥æ—¶å‘ç°ï¼‰"
                    status_fix_count += 1
                continue

            # æƒ…å†µ Bï¼šæ–‡ä»¶æœ‰ dify_document_id ä½† Dify ä¸­ä¸å­˜åœ¨ï¼Œæˆ–è€…æ²¡æœ‰ dify_document_id
            # â†’ éœ€è¦é‡æ–°ä¸Šä¼ 
            if f.status in ("indexed", "indexing", "uploading"):
                # æ‰¾åˆ°æœ¬åœ° Markdown æˆ–åŸå§‹æ–‡ä»¶
                file_content = None
                file_name = f.name
                file_path = None

                # ä¼˜å…ˆä½¿ç”¨ Markdown æ–‡ä»¶
                if f.md_file_path and Path(f.md_file_path).exists():
                    file_path = Path(f.md_file_path)
                    file_content = file_path.read_bytes()
                    # Dify ä¸Šä¼ æ—¶ä½¿ç”¨ .md æ‰©å±•å
                    if not file_name.endswith(".md"):
                        file_name = file_name.rsplit(".", 1)[0] + ".md" if "." in file_name else file_name + ".md"
                elif f.file_path and Path(f.file_path).exists():
                    file_path = Path(f.file_path)
                    file_content = file_path.read_bytes()

                if file_content:
                    logger.info(f"  ğŸ“„ é‡æ–°ä¸Šä¼ æ–‡ä»¶åˆ° Dify: '{f.name}'")
                    try:
                        upload_result = await dify.upload_document(
                            dataset_id=dataset_id,
                            file_name=file_name,
                            file_content=file_content,
                            file_type=f.file_type or "md",
                        )
                        f.dify_document_id = upload_result.document_id
                        f.dify_batch_id = upload_result.batch_id
                        f.status = "indexing"
                        reupload_count += 1

                        # å¯åŠ¨åå°è½®è¯¢ç´¢å¼•çŠ¶æ€
                        asyncio.create_task(
                            _poll_indexing_after_sync(f.id, dataset_id, upload_result.batch_id)
                        )
                    except Exception as e:
                        logger.warning(f"  âš ï¸  é‡æ–°ä¸Šä¼ å¤±è´¥ '{f.name}': {e}")
                else:
                    logger.warning(
                        f"  âš ï¸  æ–‡ä»¶ '{f.name}' æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•é‡æ–°ä¸Šä¼ åˆ° Dify"
                    )

        except Exception as e:
            logger.error(f"âŒ åŒæ­¥æ–‡ä»¶ '{f.name}' å¤±è´¥: {e}")

    await session.flush()

    if reupload_count or status_fix_count:
        logger.info(
            f"  ğŸ“Š æ–‡ä»¶åŒæ­¥: é‡æ–°ä¸Šä¼  {reupload_count} ä¸ª, çŠ¶æ€ä¿®æ­£ {status_fix_count} ä¸ª"
        )


async def _cleanup_orphan_datasets(
    session: AsyncSession,
    dify,
    dify_dataset_map: dict[str, DifyDatasetItem],
):
    """
    æ¸…ç† Dify ä¸Šçš„å­¤ç«‹ Datasetï¼ˆæœ¬åœ°æ— å¯¹åº”é›†åˆï¼‰ã€‚
    
    åœºæ™¯ï¼š
    - æœ¬åœ°é›†åˆå·²åˆ é™¤ï¼Œä½† Dify ä¾§çš„ Dataset æ®‹ç•™
    - æ³¨æ„ï¼šåªæ¸…ç†åç§°ä»¥ç‰¹å®šå‰ç¼€å¼€å¤´çš„ Datasetï¼Œé¿å…è¯¯åˆ ç”¨æˆ·æ‰‹åŠ¨åˆ›å»ºçš„
    """
    # è·å–æœ¬åœ°æ‰€æœ‰å·²å…³è”çš„ dify_dataset_id
    result = await session.execute(
        select(KBCollection.dify_dataset_id).where(KBCollection.dify_dataset_id.isnot(None))
    )
    local_dataset_ids = {row[0] for row in result.all()}

    orphan_count = 0
    for dataset_id, ds_item in dify_dataset_map.items():
        if dataset_id not in local_dataset_ids:
            logger.info(
                f"  ğŸ—‘ï¸  Dify å­¤ç«‹ Dataset: '{ds_item.name}' ({dataset_id})ï¼Œæ­£åœ¨æ¸…ç†..."
            )
            try:
                await dify.delete_dataset(dataset_id)
                orphan_count += 1
            except Exception as e:
                logger.warning(f"  âš ï¸  æ¸…ç†å­¤ç«‹ Dataset å¤±è´¥: {e}")

    if orphan_count:
        logger.info(f"  ğŸ“Š æ¸…ç†äº† {orphan_count} ä¸ªå­¤ç«‹ Dataset")


async def _poll_indexing_after_sync(
    file_id: UUID,
    dataset_id: str,
    batch_id: str,
    max_retries: int = 60,
    interval: float = 3.0,
):
    """åŒæ­¥é‡æ–°ä¸Šä¼ åçš„ç´¢å¼•çŠ¶æ€è½®è¯¢ï¼ˆä¸ knowledge.py ä¸­çš„é€»è¾‘ä¸€è‡´ï¼‰"""
    dify = get_dify_service()

    for attempt in range(max_retries):
        await asyncio.sleep(interval)
        try:
            status = await dify.get_indexing_status(dataset_id, batch_id)
        except Exception:
            continue

        if status in ("completed", "indexed"):
            async with AsyncSessionLocal() as session:
                result = await session.execute(select(KBFile).where(KBFile.id == file_id))
                kb_file = result.scalar_one_or_none()
                if kb_file:
                    kb_file.status = "indexed"
                    await session.commit()
            logger.info(f"  âœ… åŒæ­¥ä¸Šä¼ æ–‡ä»¶ç´¢å¼•å®Œæˆ [file_id={file_id}]")
            return

        if status == "error":
            async with AsyncSessionLocal() as session:
                result = await session.execute(select(KBFile).where(KBFile.id == file_id))
                kb_file = result.scalar_one_or_none()
                if kb_file:
                    kb_file.status = "failed"
                    kb_file.error_message = "Dify ç´¢å¼•å¤±è´¥ï¼ˆåŒæ­¥é‡ä¼ åï¼‰"
                    await session.commit()
            return

    # è¶…æ—¶
    async with AsyncSessionLocal() as session:
        result = await session.execute(select(KBFile).where(KBFile.id == file_id))
        kb_file = result.scalar_one_or_none()
        if kb_file and kb_file.status == "indexing":
            kb_file.status = "failed"
            kb_file.error_message = f"ç´¢å¼•è¶…æ—¶ï¼ˆåŒæ­¥é‡ä¼ åï¼Œ{max_retries * interval:.0f}ç§’ï¼‰"
            await session.commit()
